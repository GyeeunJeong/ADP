{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_나이브베이즈.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgcRSxImKDYFHM09TA4voI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joyfulspace/ADP/blob/master/10_%EB%82%98%EC%9D%B4%EB%B8%8C%EB%B2%A0%EC%9D%B4%EC%A6%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 나이브 베이즈\n",
        "* A와 B가 일어날 확률\n",
        "  - 두 사건이 독립이라면: $P(A)XP(B)$\n",
        "  - A가 일어났다는 조건 하에서 B가 일어날 확률 $P(B|A)$\n",
        "  - A와 B가 연달아서 일어날 확률: $P(A)*P(B|A)=P(A\\cap B)$ (A가 사전, B가 사후)\n",
        "  - B가 일어나고 A가 일어날 확률: $P(B)*P(A|B)=P(A\\cap B)$\n",
        "  - 따라서 $P(A)*P(B|A)=P(B)*P(A|B)$ : 베이즈 정리\n",
        "* 나이브 베이즈: A가 일어나고 B가 일어날 확률과 C가 일어나고 B가 일어날 확률은 다른데 이 원리를 이용\n",
        "  - 긍정, 부정을 예측하거나 주제 분류, 텍스트 마이닝에서 많이 사용\n",
        "  - ex. 스팸메일 찾기\n",
        "    - $P(스팸메일|'광고')=P('광고'|스팸메일)*P(스팸메일)/P('광고')$\n",
        "    - 광고 단어가 있을 때 스팸메일일 확률 = 스팸메일인데 광고 단어가 있을 확률 * 스팸메일일 확률 / '광고' 단어가 있을 확률\n",
        "   - 여러개의 단어가 나타났을 때 스팸일지 정상일지?\n",
        "   - 긴 텍스트라도 쪼개서 확률 계산 가능"
      ],
      "metadata": {
        "id": "R92eSiKWQWyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "%pylab inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAfEGwDXWRy_",
        "outputId": "3bcc0bac-0294-4dff-db85-45e2d69d956e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/joyfulspace/ADP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avXFpVLQWp-5",
        "outputId": "ee2518af-5e49-4f1a-8249-d825287d6446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ADP'...\n",
            "remote: Enumerating objects: 2488, done.\u001b[K\n",
            "remote: Counting objects: 100% (2181/2181), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2175/2175), done.\u001b[K\n",
            "remote: Total 2488 (delta 23), reused 2135 (delta 4), pack-reused 307\u001b[K\n",
            "Receiving objects: 100% (2488/2488), 55.99 MiB | 24.73 MiB/s, done.\n",
            "Resolving deltas: 100% (145/145), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n",
        "\n",
        "# 영화리뷰데이터 # 파일 없음\n",
        "with open('ADP/data/imdb_master.csv', 'rb') as f:\n",
        "  result = chardet.detect(f.read()) # 인코딩 자동으로 감지\n",
        "\n",
        "train = pd.read_csv('ADP/data/imdb_master.csv', encoding=result['encoding'])\n",
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "JYLK8BkDWUcv",
        "outputId": "009f6fb7-6a58-432c-9837-714df2831ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c1e205c23790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 영화리뷰데이터\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ADP/data/imdb_master.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchardet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 인코딩 자동으로 감지\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ADP/data/imdb_master.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop(train.columns[[0, 1, 4]], axis=1)"
      ],
      "metadata": {
        "id": "Ixq910ByWtCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.label = [(l!='neg')*1 for l in train.label]"
      ],
      "metadata": {
        "id": "9-BVVHpXXSDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "zSF5CRC8XhDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = train\n",
        "d.columns = ['user_review', 'positive']\n",
        "d.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "szY1i_a4XiPe",
        "outputId": "7369d487-1d4c-48d2-a5d1-ede52076fa85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a768b5cf58c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'user_review'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'positive'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train셋, test셋 나눔\n",
        "split = 0.7\n",
        "d_train = d[:int(split*len(d))]\n",
        "d_test = d[int((1-split)*len(d)):]"
      ],
      "metadata": {
        "id": "NJxXuX8KXoKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 나이브베이즈"
      ],
      "metadata": {
        "id": "QsDtHjr4NTHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def get_filtering(text): # feature를 바꿈\n",
        "  text = re.sub('[^A-Za-z +', '', text)\n",
        "  texts = nltk.word_tokenize(text)\n",
        "  return (texts)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(tokenizer=get_filtering)\n",
        "features = vectorizer.fit_transform(d_train.user_review)"
      ],
      "metadata": {
        "id": "bxP_64XvFn1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 30000 # 3만건에서 100번 째 칸까지 확인하기\n",
        "j = 100\n",
        "words = vectorizer.get_feature_names()[i:i+30]\n",
        "pd.DataFrame(features[j:j+10, i:i+30].todense(), columns=words)"
      ],
      "metadata": {
        "id": "SUE1DrdkF2uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model1 = MultinomialNB()\n",
        "model1.fit(features, d_train.positive)\n",
        "\n",
        "pred1 = model1.predict_proba(vectorizer.transform(d_test.user_review))\n",
        "pred1, pred1.shape"
      ],
      "metadata": {
        "id": "1IaOXlimGSYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, roc_curve\n",
        "from matplotlib.pyplot import plot, xlabel, ylabel, annotate, xlim, ylim, title\n",
        "def performance(y_true, pred, color='g', ann=True):\n",
        "  acc = accuracy_score(y_true, pred[:,1]>0.5)\n",
        "  auc = roc_auc_score(y_true, pred[:,1])\n",
        "  fpr, tpr, thr = roc_curve(y_true, pred[:,1])\n",
        "  plot(fpr, tpr, color, linewidth='3')\n",
        "  xlabel('False positive rate')\n",
        "  ylabel('True positive rate')\n",
        "  if ann:\n",
        "    annotate('Acc: %0.2f' % acc, (0.1,0.8), size=14)\n",
        "    annotate('AUC: %0.2f' % acc, (0.1,0.7), size=14)"
      ],
      "metadata": {
        "id": "1dyXJm3EGhjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance(d_test.positive, pred1)"
      ],
      "metadata": {
        "id": "R7oc-p7QHb6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 나이브베이즈 + Tf-idf"
      ],
      "metadata": {
        "id": "9XbqyjSoNYhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(tokenizer=get_filtering)\n",
        "features = vectorizer.fit_transform(d_train.user_review)"
      ],
      "metadata": {
        "id": "M7XwLBxVNb-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.transform(d_test.user_review)"
      ],
      "metadata": {
        "id": "tJ7lxQnWPLuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 비교\n",
        "pred2 = model1.predict_proba(vectorizer.transform(d_test.user_review))\n",
        "performance(d_test.positive, pred1, ann=False)\n",
        "performance(d_test.positive, pred2, color='b') # tf-idf한 것이 결과 더 좋음\n",
        "xlim(0, 0.5)\n",
        "ylim(0.5, 1)"
      ],
      "metadata": {
        "id": "f6DEKKdQPREL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 매개변수 최적화\n",
        "# 관찰할 옵션의 정의\n",
        "param_ranges = {\n",
        "    'max_features': [10000, 30000, 50000, None],\n",
        "    'min_df': [1, 2, 3],\n",
        "    'nb_alpha':[0.01, 0.1, 1.0]\n",
        "}"
      ],
      "metadata": {
        "id": "PEovr_myPgmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 옵션을 주고 머징모델을 만들어 주는 함수\n",
        "def build_model(max_features=None, min_df=1, nb_alpha=1.0, return_preds=False):\n",
        "  vectorizer = TfidfVectorizer(max_features=max_features, min_df=min_df, tokenizer=get_filtering)\n",
        "  features = vectorizer.fit_transform(d_train.user_review)\n",
        "  model = MultinomialNB(alpha=nb_alpha)\n",
        "  model.fit(features, d_train.positive)\n",
        "  pred = model.predict_proba(vectorizer.transform(d_test.user_review))\n",
        "  res = {\n",
        "      'max_features':max_features,\n",
        "      'min_df':min_df,\n",
        "      'nb_alpha':nb_alpha,\n",
        "      'auc':roc_auc_score(d_test.positive, pred[:,1])\n",
        "  }\n",
        "  if return_preds:\n",
        "    res['preds'] = pred\n",
        "  return res"
      ],
      "metadata": {
        "id": "-bNSyRIGP2KN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 옵션을 변화하면서 만들어지는 머징모델들의 평가수치를 저장하고 출력\n",
        "from itertools import product\n",
        "results = []\n",
        "for p in product(*param_ranges.values()):\n",
        "  res = build_model(**dict(zip(param_ranges.keys(), p)))\n",
        "  results.append(res)\n",
        "  print(res)"
      ],
      "metadata": {
        "id": "1MVSOvIwQkVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과값을 데이터프레임으로 만듬\n",
        "opt = pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "iQN3RRfVSTaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mf_idx = [0, 9, 18, 27]\n",
        "plot(opt.max_features[mf_idx], opt.auc[mf_idx], linewidth=2)\n",
        "title('AUC vs max_features')"
      ],
      "metadata": {
        "id": "jPdphs35GCxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdf_idx = [1, 20, 30]\n",
        "plot(opt.nb_alpha[mdf_idx], opt.auc[mdf_idx], linewidth=2)\n",
        "title('AUC vs min_df')"
      ],
      "metadata": {
        "id": "tNX39zzSGDXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nba_idx = [1, 20, 30]\n",
        "plot(opt.nb_alpha[nba_idx], opt.auc[nba_idx], linewidth=2)\n",
        "title('AUC vs alpha')"
      ],
      "metadata": {
        "id": "rl_WAmmEGSOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3가지 융합모델\n",
        "pred3 = build_model(nb_alpha=0.01, return_preds=True)['preds']\n",
        "performance(d_test.positive, pred1, ann=False)\n",
        "performance(d_test.positive, pred2, color='b', ann=False)\n",
        "performance(d_test.positive, pred3, color='r')\n",
        "xlim(0, 0.5)\n",
        "ylim(0.5, 1)"
      ],
      "metadata": {
        "id": "nrakVXkIGMR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 랜덤 포레스트+Tf-idf"
      ],
      "metadata": {
        "id": "db6uhYNsHlAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tf-idf\n",
        "vectorizer = TfidfVectorizer(strip_accents='unicode', min_df=3, max_features=30000, norm='l2')\n",
        "features = vectorizer.fit_transform(d_train.user_review)"
      ],
      "metadata": {
        "id": "Lhk6DJkaHttz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 나이브베이즈 + Tf-idf\n",
        "model3 = MultinomialNB()\n",
        "model3.fit(features, d_train.positive)\n",
        "pred3 = model3.predict_proba(vectorizer.transform(d_test.user_review))\n",
        "performance(d_test.positive, pred3)"
      ],
      "metadata": {
        "id": "fSkkES-9IGIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤포레스트\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model2 = RandomForestClassifier(n_estimators=100)\n",
        "model2.fit(features, d_train.positive)"
      ],
      "metadata": {
        "id": "CvcTKpdvIpv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤포레스트 + Tf-idf 머징모델 평가\n",
        "pred2 = model2.predict_proba(vectorizer.transform(d_test.user_review))\n",
        "performance(d_test.positive, pred2)"
      ],
      "metadata": {
        "id": "QE2xbD7kI54y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤포레스트 + Tf-idf 머징모델의 테스트 적용\n",
        "examples = [\"First of all I hate those moronic rappers\",\n",
        "            \"She was approached with her own show... she jumped ship on Drake and Josh. They then decided that maybe they would do a \",\n",
        "            \"I found a couple of topics unusually explicitly addressed, and until the end,\",\n",
        "            \"One Great movie, I've watched it several times\",\n",
        "            \"It's a grisly movie if you are intereseted in that, and there's often a morbid focus on food to accompany events, like a \",\n",
        "            \"I think many people are annoyed with this film because it's being pushed as a horror film--which it isn't. So, if you \"\n",
        "]\n",
        "\n",
        "model2.predict(vectorizer.transform(examples))"
      ],
      "metadata": {
        "id": "KthrCpx2JMjD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. word2vec 사용\n",
        "* Doc2vec: Word2vec을 이용하여 텍스트를 단어처럼 하나의 벡터로 나타낸 것."
      ],
      "metadata": {
        "id": "d1IAV11xMY3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "PpVt97vLN93S",
        "outputId": "eac57ca4-b5b5-4dcf-e20e-e83b086332c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 긍정/부정과 관련없은 영화배우 이름 추가(주관적인 작업)\n",
        "stop_words = set(['Tom Cruise', 'Johansson', 'Reynolds', 'Nicole Kidman', 'DiCaprio'])\n",
        "\n",
        "# 띄어쓰기와 알파벳 활용\n",
        "def tokenize(docs):\n",
        "  pattern = re.compile('[\\W_]+', re.UNICODE)\n",
        "  sentences = []\n",
        "\n",
        "  for d in docs:\n",
        "    sentence = d.lower().split(\" \")\n",
        "    sentence = [pattern.sub('', w) for w in sentence]\n",
        "    sentences.append([w for w in sentence if w not in stop_words])\n",
        "\n",
        "  return sentences\n",
        "\n",
        "# 1글자 이상의 명사, 동사, 형용사, 부사 등을 추출함.\n",
        "def get_noun(text):\n",
        "  text = re.sub('[^A-Za-z ]+', '', text)\n",
        "  texts = nltk.word_tokenize(text)\n",
        "  return ([x for (x,y) in (nltk.pos_tag(texts)) if (y=='NNP' or y=='NN' or y=='PRP' or y=='VBP' or y=='RB' or y=='JJ') and len(x)>0])\n",
        "\n",
        "# 명사, 동사, 형용사, 부사 추출\n",
        "def tokenize2(docs):\n",
        "  results = []\n",
        "  for d in docs:\n",
        "    r = get_noun(d)\n",
        "    results.append(r)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "U6ULWRNsLm9m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(examples)"
      ],
      "metadata": {
        "id": "efzuQ5_uNrgv",
        "outputId": "7102cf5b-8e5b-4464-9421-32c13a21f02c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['first', 'of', 'all', 'i', 'hate', 'those', 'moronic', 'rappers'],\n",
              " ['she',\n",
              "  'was',\n",
              "  'approached',\n",
              "  'with',\n",
              "  'her',\n",
              "  'own',\n",
              "  'show',\n",
              "  'she',\n",
              "  'jumped',\n",
              "  'ship',\n",
              "  'on',\n",
              "  'drake',\n",
              "  'and',\n",
              "  'josh',\n",
              "  'they',\n",
              "  'then',\n",
              "  'decided',\n",
              "  'that',\n",
              "  'maybe',\n",
              "  'they',\n",
              "  'would',\n",
              "  'do',\n",
              "  'a',\n",
              "  ''],\n",
              " ['i',\n",
              "  'found',\n",
              "  'a',\n",
              "  'couple',\n",
              "  'of',\n",
              "  'topics',\n",
              "  'unusually',\n",
              "  'explicitly',\n",
              "  'addressed',\n",
              "  'and',\n",
              "  'until',\n",
              "  'the',\n",
              "  'end'],\n",
              " ['one', 'great', 'movie', 'ive', 'watched', 'it', 'several', 'times'],\n",
              " ['its',\n",
              "  'a',\n",
              "  'grisly',\n",
              "  'movie',\n",
              "  'if',\n",
              "  'you',\n",
              "  'are',\n",
              "  'intereseted',\n",
              "  'in',\n",
              "  'that',\n",
              "  'and',\n",
              "  'theres',\n",
              "  'often',\n",
              "  'a',\n",
              "  'morbid',\n",
              "  'focus',\n",
              "  'on',\n",
              "  'food',\n",
              "  'to',\n",
              "  'accompany',\n",
              "  'events',\n",
              "  'like',\n",
              "  'a',\n",
              "  ''],\n",
              " ['i',\n",
              "  'think',\n",
              "  'many',\n",
              "  'people',\n",
              "  'are',\n",
              "  'annoyed',\n",
              "  'with',\n",
              "  'this',\n",
              "  'film',\n",
              "  'because',\n",
              "  'its',\n",
              "  'being',\n",
              "  'pushed',\n",
              "  'as',\n",
              "  'a',\n",
              "  'horror',\n",
              "  'filmwhich',\n",
              "  'it',\n",
              "  'isnt',\n",
              "  'so',\n",
              "  'if',\n",
              "  'you',\n",
              "  '']]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize2(examples) # 더 체계적인 형태"
      ],
      "metadata": {
        "id": "Z6h_Xl04NxS8",
        "outputId": "e33f6fc7-3435-4a81-c238-1acf56065039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['First', 'I', 'hate', 'moronic'],\n",
              " ['She',\n",
              "  'own',\n",
              "  'show',\n",
              "  'she',\n",
              "  'ship',\n",
              "  'Drake',\n",
              "  'Josh',\n",
              "  'They',\n",
              "  'then',\n",
              "  'maybe',\n",
              "  'they'],\n",
              " ['I', 'couple', 'unusually', 'explicitly', 'end'],\n",
              " ['Great', 'movie', 'Ive', 'it', 'several'],\n",
              " ['grisly', 'movie', 'you', 'are', 'often', 'morbid', 'focus', 'food'],\n",
              " ['I',\n",
              "  'think',\n",
              "  'many',\n",
              "  'are',\n",
              "  'film',\n",
              "  'horror',\n",
              "  'filmwhich',\n",
              "  'it',\n",
              "  'So',\n",
              "  'you']]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = tokenize(d_train.user_review)\n",
        "sentences2 = tokenize2(d_train.user_review)\n",
        "len(sentences), len(sentences2)"
      ],
      "metadata": {
        "id": "Fx_fOzCAN6vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "model = Word2Vec(sentences2, size=300, window=10, min_count=1, sample=1e-3, workers=2) #size: word를 벡터로 바꿀 때 벡터의 크기, window: 한 단어와 가까운 주변 단어의 크기, workers: 스레드"
      ],
      "metadata": {
        "id": "LR42my58N2pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.inti_sims(replace=True) # 트레이닝이 완료되면 init_sims 명령으로 필요없는 메모리를 unload 시킨다."
      ],
      "metadata": {
        "id": "pxMjPDgGPLGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word2vec을 이용하여 텍스트를 수치화. doc2vec\n",
        "from numpy import zeros\n",
        "def featurize_w2v(model, sentences):\n",
        "  f = zeros((len(sentences), model.vector_size))\n",
        "  for i, s in enumerate(sentences):\n",
        "    for w in s:\n",
        "      try:\n",
        "        vec = model[w]\n",
        "      except KeyError:\n",
        "        continue\n",
        "      f[i,:] = f[i,:] + vec\n",
        "    f[i,:] = f[i,:]/len(s)\n",
        "  return f"
      ],
      "metadata": {
        "id": "mrg8v5vgPVn_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_w2v = featurize_w2v(model, sentences)\n",
        "features_w2v.shape, type(features_w2v) # (70000, 300), numpy.ndarray"
      ],
      "metadata": {
        "id": "IYNExk6zPvtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#doc2vec + 랜덤포레스트 머징모델\n",
        "model4 = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
        "model4.fit(features_w2v, d_train.positive)"
      ],
      "metadata": {
        "id": "dyBc0X1jP8EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize2 테스트\n",
        "test_sentences = tokenize(d_test.user_review)"
      ],
      "metadata": {
        "id": "oMx37HnFSD0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features_w2v = featurize_w2v(model, test_sentences) # sentences를 벡터로 만듦"
      ],
      "metadata": {
        "id": "tgB7OsQdSLzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# doc2vec + 랜덤포레스트 머징모델을 7만개의 문장에 대해 테스트해본다\n",
        "pred4 = model4.predict_proba(test_features_w2v)\n",
        "pred4.shape # 70000, 2 긍정/부정"
      ],
      "metadata": {
        "id": "rsr-3oXZSbf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트한 결과(긍정 or 부정)\n",
        "for i in range(0, pred4.shape[0]):\n",
        "  positive_true = '부정' if (pred4[i][0] > pred4[i][1]) else '긍정'\n",
        "  print(i, positive_true)"
      ],
      "metadata": {
        "id": "NmL30TH0Swvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# doc2vec + 랜덤포레스트 머징모델 평가 그래프\n",
        "performance(list(d_test.positive), pred4, color='b')\n",
        "xlim(0, 1)\n",
        "ylim(0, 1)"
      ],
      "metadata": {
        "id": "7SeodA-STGna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# doc2vec _ 랜덤포레스트 머징모델 테스트\n",
        "example_feat4 = featurize_w2v(model, tokenize2(examples))\n",
        "model4.predict(example_feat4)\n",
        "example_feat4.shape # 6, 300 6개 문장에 300개 벡터"
      ],
      "metadata": {
        "id": "BNclNjHcThfk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}